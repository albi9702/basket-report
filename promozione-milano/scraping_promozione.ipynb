{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79368444",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Scraping Ado\n",
    "\n",
    "This script shows how to scrape data from [LombardiaCanestro](https://lombardia.italiacanestro.it/). In this case, scrape the **Promozione - Girone E League**. Here the list of all Teams:\n",
    "\n",
    "* [Aurora Trezzo](https://lombardia.italiacanestro.it/Maschile/Squadra?id=42&tid=482).\n",
    "* [Posal Sesto San Giovanni](https://lombardia.italiacanestro.it/Maschile/Squadra?id=42&tid=483).\n",
    "* [Ado San Benedetto Milano](https://lombardia.italiacanestro.it/Maschile/Squadra?id=42&tid=484).\n",
    "* [CGB Brugherio](https://lombardia.italiacanestro.it/Maschile/Squadra?id=42&tid=485).\n",
    "* [Azzurri Niguardese](https://lombardia.italiacanestro.it/Maschile/Squadra?id=42&tid=486).\n",
    "* [Pallacanestro Carugate](https://lombardia.italiacanestro.it/Maschile/Squadra?id=42&tid=487).\n",
    "* [CBBA Olimpia Cologno](https://lombardia.italiacanestro.it/Maschile/Squadra?id=42&tid=488).\n",
    "* [Cesano Seveso](https://lombardia.italiacanestro.it/Maschile/Squadra?id=42&tid=489).\n",
    "* [Inzago Basket](https://lombardia.italiacanestro.it/Maschile/Squadra?id=42&tid=490).\n",
    "* [OSAL Novate](https://lombardia.italiacanestro.it/Maschile/Squadra?id=42&tid=491).\n",
    "* [Basket Ajaccio 1988](https://lombardia.italiacanestro.it/Maschile/Squadra?id=42&tid=492).\n",
    "* [Social OSA](https://lombardia.italiacanestro.it/Maschile/Squadra?id=42&tid=493).\n",
    "* [Basket San Rocco 2013 Seregno](https://lombardia.italiacanestro.it/Maschile/Squadra?id=42&tid=494).\n",
    "* [Ciesse Freebasket Milano](https://lombardia.italiacanestro.it/Maschile/Squadra?id=42&tid=495).\n",
    "* [ACLI Trecella](https://lombardia.italiacanestro.it/Maschile/Squadra?id=42&tid=496)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d967655b",
   "metadata": {},
   "source": [
    "## Python Script\n",
    "\n",
    "The following cells shows the scrape script from the LombardiaCanestro Site. Scrape data:\n",
    "\n",
    "* *All games*, with this link: `https://lombardia.italiacanestro.it/Maschile/Partita?id=`***`<id_game>`***\n",
    "* *Standings*, with this link: `https://lombardia.italiacanestro.it/Maschile/Calendario?id=`***`42`***\n",
    "* *Rosters*, with this link: `https://lombardia.italiacanestro.it/Maschile/Roster?id=`***`42`***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7594cd8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Libraries\n",
    "\n",
    "Used libraries:\n",
    "\n",
    "* `pandas`.\n",
    "* `pycelonis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5554c800",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-06-04 09:04:32,953] WARNING: Your PyCelonis Version 2.0.0 is outdated (Newest Version: 2.2.0). Please upgrade the package via: pip install --extra-index-url=https://pypi.celonis.cloud/ pycelonis --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-04 09:04:33,159] INFO: Initial connect successful! PyCelonis Version: 2.0.0\n",
      "[2023-06-04 09:04:33,356] INFO: `package-manager` permissions: ['EDIT_ALL_SPACES', 'MANAGE_PERMISSIONS', 'CREATE_SPACE', 'DELETE_ALL_SPACES']\n",
      "[2023-06-04 09:04:33,357] INFO: `workflows` permissions: ['EDIT_AGENTS', 'VIEW_AGENTS', 'REGISTER_AGENTS', 'MANAGE_PERMISSIONS']\n",
      "[2023-06-04 09:04:33,357] INFO: `task-mining` permissions: ['EDIT_CLIENT_SETTINGS', 'EDIT_USERS']\n",
      "[2023-06-04 09:04:33,358] INFO: `action-engine` permissions: ['CREATE_PROJECTS', 'MANAGE_SKILLS', 'ACCESS_ALL_PROJECTS', 'MY_INBOX']\n",
      "[2023-06-04 09:04:33,359] INFO: `team` permissions: ['MANAGE_AUDIT_LOGS', 'MANAGE_SSO_SETTINGS', 'USE_AUDIT_LOGS_API', 'MANAGE_ADOPTION_VIEWS', 'MANAGE_GENERAL_SETTINGS', 'MANAGE_GROUPS', 'MANAGE_APPLICATIONS', 'USE_STUDIO_ADOPTION_API', 'MANAGE_LOGIN_HISTORY', 'MANAGE_LICENSE_SETTINGS', 'USE_LOGIN_HISTORY_API', 'MANAGE_MEMBERS', 'MANAGE_UPLINK_INTEGRATIONS', 'MANAGE_PERMISSIONS', 'MANAGE_ADMIN_NOTIFICATIONS', 'IMPORT_MEMBERS', 'MANAGE_MEMBER_LOCKING_POLICY']\n",
      "[2023-06-04 09:04:33,360] INFO: `process-repository` permissions: ['CREATE_AND_MODIFY_CATEGORIES', 'USE_CATEGORIES', 'DELETE_EXISTING_CATEGORIES', 'MODIFY_EXISTING_CATEGORIES']\n",
      "[2023-06-04 09:04:33,361] INFO: `process-analytics` permissions: ['CREATE_WORKSPACE', 'MOVE_TO', 'DELETE_ALL_WORKSPACES', 'DELETE_ALL_ANALYSES', 'EDIT_ALL_ANALYSES', 'EDIT_ALL_WORKSPACES', 'USE_ALL_ANALYSES', 'CREATE_ANALYSES', 'MANAGE_PERMISSIONS', 'EXPORT_CONTENT']\n",
      "[2023-06-04 09:04:33,362] INFO: `transformation-center` permissions: ['MOVE_TO', 'EDIT_OBJECTIVE', 'VIEW_OBJECTIVE', 'CREATE_OBJECTIVE', 'MANAGE_PERMISSIONS', 'CREATE_KPI', 'EXPORT_CONTENT', 'DELETE_OBJECTIVE']\n",
      "[2023-06-04 09:04:33,362] INFO: `storage-manager` permissions: ['DELETE', 'CREATE', 'GET', 'ADMIN', 'LIST']\n",
      "[2023-06-04 09:04:33,365] INFO: `event-collection` permissions: ['EDIT_ALL_DATA_POOLS_RESTRICTED', 'USE_ALL_DATA_MODELS', 'VIEW_ALL_DATA_POOLS', 'CREATE_DATA_POOL', 'EDIT_ALL_DATA_POOLS']\n",
      "[2023-06-04 09:04:33,366] INFO: `user-provisioning` permissions: ['SCIM']\n",
      "[2023-06-04 09:04:33,367] INFO: `ml-workbench` permissions: ['DELETE_SCHEDULERS', 'EDIT_SCHEDULERS', 'USE_ALL_SCHEDULERS', 'USE_ALL_APPS', 'CREATE_SCHEDULERS', 'MANAGE_ALL_APPS', 'CREATE_WORKSPACES', 'MANAGE_SCHEDULERS_PERMISSIONS', 'VIEW_CONFIGURATION', 'CREATE_APPS', 'MANAGE_ALL_MLFLOWS', 'CREATE_MLFLOWS', 'USE_ALL_MLFLOWS', 'MANAGE_ALL_WORKSPACES']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from   urllib.request import Request, urlopen\n",
    "from   tqdm import tqdm\n",
    "\n",
    "#-- Celonis\n",
    "from pycelonis import get_celonis\n",
    "celonis = get_celonis(\n",
    "    base_url = \"alberto-filosa-protiviti-it.training.celonis.cloud\",\n",
    "    api_token = \"NzQ4Mzg3YjctNzkzNy00ZTFhLWE5ZTUtN2Y5NDk0MGVhYWJiOnlHK2xYb3NKRHpwTitGU053NUxOT2ZDZFZOUllKaXNsNWlUeGFwVnJ0UTc3\",\n",
    "    key_type = 'USER_KEY'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ccfc34",
   "metadata": {},
   "source": [
    "### Scraping Webpage\n",
    "\n",
    "The `scraping_webpage` function takes in input the URL of the page to scrape the data (treated as *string*). It returns a DataFrame of the URL table.\n",
    "\n",
    "This function is nested in the following tables:\n",
    "\n",
    "* `scraping_table`.\n",
    "* `scraping_players`.\n",
    "* `scraping_standings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "910a1d5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scraping_webpage(lv_url, link = None):\n",
    "        \n",
    "    #-- Get URL\n",
    "    req = Request(lv_url, headers = {'User-Agent': 'Mozilla/5.0'})\n",
    "    webpage = urlopen(req).read()\n",
    "\n",
    "    #-- Get Table in HTML\n",
    "    df_scraped = pd.read_html(webpage, extract_links = link)\n",
    "    \n",
    "    return df_scraped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb2ff01",
   "metadata": {},
   "source": [
    "### Games Scraping\n",
    "\n",
    "<!-- Inserire cosa fa la funzione -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e6f6ac9-d483-4c8d-ab0e-8cb7f0cdf4b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scraping_table(ls_id_game):\n",
    "    #-- Disable chained assignments\n",
    "    pd.options.mode.chained_assignment = None \n",
    "\n",
    "    #-- Read the existing scraped data from the CSV file (if it exists)\n",
    "    try:\n",
    "        existing_data = pd.read_csv('scraped_data.csv')\n",
    "    except FileNotFoundError:\n",
    "        existing_data = pd.DataFrame()\n",
    "\n",
    "    #-- Initialize an empty DataFrame to store the scraped data\n",
    "    df_all_games = pd.DataFrame()\n",
    "\n",
    "    for game in tqdm(ls_id_game):\n",
    "        \n",
    "        #-- Check if any data already exists\n",
    "        if len(existing_data) > 0:\n",
    "            #-- Check if the ID has already been scraped\n",
    "            if len(existing_data[existing_data['id_gara'] == game]) > 0:\n",
    "                continue\n",
    "\n",
    "        lv_url = f\"https://lombardia.italiacanestro.it/Maschile/Partita?id={game}\"\n",
    "        \n",
    "        #-- Call the scraping_webpage function to scrape the Team Table\n",
    "        df_single_game = scraping_webpage(lv_url)[0]\n",
    "        \n",
    "        #-- If not played yet, move to the next game\n",
    "        if len(df_single_game.index) < 5:\n",
    "            continue\n",
    "        \n",
    "        #-- List of Teams\n",
    "        ls_teams = df_single_game.loc[df_single_game[1] == \"PTS\"][0]\n",
    "        \n",
    "        #-----------------------\n",
    "        #-- Data Manipulation --\n",
    "        #-----------------------\n",
    "        \n",
    "        #-- Drop NAs (in a single URL there is one single Table to identify the Teams)\n",
    "        df_single_game_nona = df_single_game.dropna(how = \"all\")\n",
    "        \n",
    "        #-- Add Columns in the DataFrame\n",
    "        df_single_game_nona[\"Squadra\"]    = [ls_teams[0] if ls_teams.index[1] > row else ls_teams[ls_teams.index[1]] for row in range(0, df_single_game_nona.shape[0])]\n",
    "        df_single_game_nona[\"Squadra\"]    = df_single_game_nona[\"Squadra\"].str.title()\n",
    "        df_single_game_nona[\"Avversario\"] = [ls_teams[ls_teams.index[1]] if ls_teams.index[1] > row else ls_teams[0] for row in range(0, df_single_game_nona.shape[0])]\n",
    "        df_single_game_nona[\"Avversario\"] = df_single_game_nona[\"Avversario\"].str.title()\n",
    "        df_single_game_nona[\"Partita\"]    = [\"C\" if ls_teams.index[1] > row else \"T\" for row in range(0, df_single_game_nona.shape[0])]\n",
    "        df_single_game_nona[\"id_gara\"]    = game\n",
    "        \n",
    "        #-- Remove Header Rows (if they have PTS in the first column)\n",
    "        df_single_game_end = df_single_game_nona[df_single_game_nona[1] != 'PTS']\n",
    "        df_single_game_end.columns = [\"giocatore\", \"punti_totali\", \"tiri_liberi\",\n",
    "                                      \"due_punti\", \"tre_punti\", \"squadra\",\n",
    "                                      \"avversario\", \"partita\",\"id_gara\"]\n",
    "        \n",
    "        df_single_game_end[\"giocatore\"] = df_single_game_end[\"giocatore\"].str.title()\n",
    "        \n",
    "        #-- Concatenate games\n",
    "        df_all_games = pd.concat([df_all_games, df_single_game_end])\n",
    "        \n",
    "        #-- Add the scraped data to the existing data DataFrame\n",
    "        existing_data = pd.concat([existing_data, df_single_game_end])\n",
    "    \n",
    "    #-- Write the updated data to the CSV file\n",
    "    existing_data.to_csv('scraped_data.csv', index = False)\n",
    "    \n",
    "    return df_all_games"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e443e90",
   "metadata": {},
   "source": [
    "### Rosters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "395c755e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scraping_players(id_roster):\n",
    "    \n",
    "    #-- Iniziate Empty DataFrame. Then, will be inserted for each ig_game the Game Result\n",
    "    df_all_players = pd.DataFrame()\n",
    "    \n",
    "    lv_url = f\"https://lombardia.italiacanestro.it/Maschile/Roster?id={id_roster}\"\n",
    "    \n",
    "    #-- Call the scraping_webpage function to scrape the Team Table\n",
    "    df_single_team = scraping_webpage(lv_url)\n",
    "    \n",
    "    for team in tqdm(df_single_team):\n",
    "    \n",
    "        team[\"Squadra\"] = team.columns[1].title()\n",
    "        team.columns = [\"Numero\", \"Giocatore\", \"Squadra\"]\n",
    "        df_all_players = pd.concat([df_all_players, team], axis = 0, ignore_index = True)\n",
    "    \n",
    "    df_all_players[\"Giocatore\"] = df_all_players[\"Giocatore\"].str.title()\n",
    "    \n",
    "    return df_all_players"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c57317",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Schedule Scraping\n",
    "\n",
    "The `scraping_schedule` takes in input the following variables:\n",
    "\n",
    "* `id_championship`: the identifier of the Championship.\n",
    "* `ls_turn`: the identifier of the turns (Andata - 2 and Ritorno - 3 in the URL)\n",
    "* `ls_round`: the number of the single round (from 1 to 15).\n",
    "\n",
    "It returns the Dataframe containing all Schedule of the Championship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6e8b12e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scraping_schedule(id_championship, ls_turn, ls_round):\n",
    "    \n",
    "    #-- Disable chained assignments\n",
    "    pd.options.mode.chained_assignment = None \n",
    "    \n",
    "    #-- Iniziate Empty DataFrame. Then, will be inserted for each ig_game the Game Result\n",
    "    df_all_schedules  = pd.DataFrame()\n",
    "    \n",
    "    for turns in ls_turn:\n",
    "        print(f\"Scraping Round {turns - 1} ...\")\n",
    "        \n",
    "        for rounds in tqdm(ls_round):\n",
    "            \n",
    "            lv_url = f\"https://lombardia.italiacanestro.it/Maschile/Calendario?handler=Change&ChampionshipId={id_championship}&TurnId={turns}&TurnNm={rounds}\"\n",
    "            \n",
    "            df_single_round = scraping_webpage(lv_url, link = \"body\")[0]\n",
    "            df_single_round_tidy = pd.DataFrame()\n",
    "            df_single_round_tidy[\"Data\"] = df_single_round[0][df_single_round.index % 4 == 0].reset_index(drop = True).str.get(0).str.title()\n",
    "            df_single_round_tidy[\"Squadra\"] = df_single_round[0][(df_single_round.index - 1) % 4 == 0].reset_index(drop = True).str.get(0).str.replace(r'\\([^)]*\\)', '', regex = True).str.title()\n",
    "            df_single_round_tidy[\"Id_Gara\"] = df_single_round[0][(df_single_round.index + 2) % 4 == 0].reset_index(drop = True).str.get(1).str.extract(r'(\\d+)').astype(int)\n",
    "            df_single_round_tidy[\"Risultato\"] = df_single_round[1][(df_single_round.index - 1) % 4 == 0].reset_index(drop = True).str.get(0)\n",
    "            df_single_round_tidy[\"Avversario\"] = df_single_round[2][(df_single_round.index - 1) % 4 == 0].reset_index(drop = True).str.get(0).str.replace(r'\\([^)]*\\)', '', regex = True).str.title()\n",
    "            df_single_round_tidy[\"Girone\"] = turns - 1\n",
    "            df_single_round_tidy[\"Turno\"] = rounds\n",
    "            df_single_round_tidy.head()\n",
    "            \n",
    "            #-- Concat Games\n",
    "            df_all_schedules = pd.concat([df_all_schedules, df_single_round_tidy])\n",
    "            \n",
    "    return df_all_schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c6cd93",
   "metadata": {},
   "source": [
    "### Standings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51ee0f2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scraping_standings(id_calendar):\n",
    "    \n",
    "    lv_url = f\"https://lombardia.italiacanestro.it/Maschile/Calendario?id={id_calendar}\"\n",
    "\n",
    "    #-- Call the scraping_webpage function to scrape the Standing Table\n",
    "    df_standings = scraping_webpage(lv_url)[1]\n",
    "\n",
    "    #-- Data String Manipulation\n",
    "    df_standings[\"CLASSIFICA\"] = df_standings[\"CLASSIFICA\"].str.title()\n",
    "\n",
    "    df_standings.columns = [\"posizione\", \"squadra\", \"punti\", \"partite_giocate\",\n",
    "                            \"vittorie\",  \"sconfitte\", \"punti_fatti\", \"punti_subiti\"]\n",
    "\n",
    "    return df_standings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3a5c55",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Upload to Celonis\n",
    "\n",
    "| DataFrame Name     | SQL Table Name      |\n",
    "|--------------------|---------------------|\n",
    "| `df_all_games`     | `PR_DATA_GAMES`     |\n",
    "| `df_standing`      | `PR_DATA_STANDINGS` |\n",
    "| `df_all_players`   | `PR_PLAYERS_NAME`   |\n",
    "| `df_all_schedules` | `PR_SCHEDULES`      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b6956e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading All Games ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 206/206 [00:45<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading All Players ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 1192.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Standins ...\n",
      "Downloading All Schedule ...\n",
      "Scraping Round 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:05<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Round 2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:05<00:00,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.97 s, sys: 395 ms, total: 7.36 s\n",
      "Wall time: 56.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#-----------------------#\n",
    "#-- Scraping Girone E --#\n",
    "#-----------------------#\n",
    "\n",
    "print(\"Downloading All Games ...\")\n",
    "df_all_games   = scraping_table(np.arange(5508, 5714))\n",
    "\n",
    "print(\"Downloading All Players ...\")\n",
    "df_all_players = scraping_players(42)\n",
    "\n",
    "print(\"Downloading Standins ...\")\n",
    "df_standing    = scraping_standings(42)\n",
    "\n",
    "print(\"Downloading All Schedule ...\")\n",
    "df_all_schedules = scraping_schedule(42, np.arange(2, 4), np.arange(1, 16))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad86c58-9ef8-48f1-b913-e545c050c0df",
   "metadata": {
    "tags": []
   },
   "source": [
    "%%time\n",
    "\n",
    "```py\n",
    "#-----------------------#\n",
    "#-- Scraping Girone G --#\n",
    "#-----------------------#\n",
    "\n",
    "print(\"Downloading All Games ...\")\n",
    "df_all_games   = scraping_table(np.arange(5958, 6196))\n",
    "\n",
    "print(\"Downloading All Players ...\")\n",
    "df_all_players = scraping_players(44)\n",
    "\n",
    "print(\"Downloading Standins ...\")\n",
    "df_standing    = scraping_standings(44)\n",
    "\n",
    "print(\"Downloading All Schedule ...\")\n",
    "df_all_schedules = scraping_schedule(44, np.arange(2, 4), np.arange(1, 16))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94baf95d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected the 'Basket - Scraping Data' Data Pool and the 'Data Model - Promozione - Girone E' Data Model\n"
     ]
    }
   ],
   "source": [
    "#-- Selecting Data Pool, Data Model and Data Job\n",
    "data_pool = celonis.data_integration.get_data_pool(\"26a8fa87-21b1-4850-9447-48c2e6a171fc\")\n",
    "data_model = data_pool.get_data_model(\"9fb8576b-a8f6-4f71-9cb9-4722bafa7a92\")\n",
    "print(f\"Selected the '{data_pool.name}' Data Pool and the '{data_model.name}' Data Model\")\n",
    "\n",
    "data_job = data_pool.get_job(\"f9300adf-cde5-43d8-bc44-eca7b355fda1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb640f5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading of the 'PR_DATA_GAMES' Table from Python to Celonis: \n",
      "\n",
      "[2023-06-04 09:05:43,596] INFO: Successfully created data push job with id '0101c0c5-2286-4c92-98ef-0d7f15410f4a'\n",
      "[2023-06-04 09:05:43,597] INFO: Add data frame as file chunks to data push job with id '0101c0c5-2286-4c92-98ef-0d7f15410f4a'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9ac74c3914d4c9bae4c1584791d3958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-04 09:05:44,233] INFO: Successfully upserted file chunk to data push job with id '0101c0c5-2286-4c92-98ef-0d7f15410f4a'\n",
      "[2023-06-04 09:05:44,406] INFO: Successfully triggered execution for data push job with id '0101c0c5-2286-4c92-98ef-0d7f15410f4a'\n",
      "[2023-06-04 09:05:44,407] INFO: Wait for execution of data push job with id '0101c0c5-2286-4c92-98ef-0d7f15410f4a'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0568dafe5841409d85096af0a8d22bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-04 09:05:58,784] INFO: Successfully created table 'PR_DATA_GAMES' in data pool\n",
      "[2023-06-04 09:05:58,941] INFO: Successfully deleted data push job with id '0101c0c5-2286-4c92-98ef-0d7f15410f4a'\n",
      "Upload of the Table Completed!\n",
      "_____________________________________________ \n",
      " \n",
      "\n",
      "Uploading of the 'PR_DATA_STANDINGS' Table from Python to Celonis: \n",
      "\n",
      "[2023-06-04 09:06:01,797] INFO: Successfully created data push job with id '2510ab1e-c3d3-42da-83ad-444942244acc'\n",
      "[2023-06-04 09:06:01,798] INFO: Add data frame as file chunks to data push job with id '2510ab1e-c3d3-42da-83ad-444942244acc'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4533e4c40ffa471aac22e0205df64734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-04 09:06:02,118] INFO: Successfully upserted file chunk to data push job with id '2510ab1e-c3d3-42da-83ad-444942244acc'\n",
      "[2023-06-04 09:06:02,296] INFO: Successfully triggered execution for data push job with id '2510ab1e-c3d3-42da-83ad-444942244acc'\n",
      "[2023-06-04 09:06:02,297] INFO: Wait for execution of data push job with id '2510ab1e-c3d3-42da-83ad-444942244acc'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86fe40310f094e58aba29bb455c3d4d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-04 09:06:18,705] INFO: Successfully created table 'PR_DATA_STANDINGS' in data pool\n",
      "[2023-06-04 09:06:18,845] INFO: Successfully deleted data push job with id '2510ab1e-c3d3-42da-83ad-444942244acc'\n",
      "Upload of the Table Completed!\n",
      "_____________________________________________ \n",
      " \n",
      "\n",
      "Uploading of the 'PR_PLAYERS_NAME' Table from Python to Celonis: \n",
      "\n",
      "[2023-06-04 09:06:21,696] INFO: Successfully created data push job with id '7cc773ce-8e00-4169-96b4-cd054caf451b'\n",
      "[2023-06-04 09:06:21,697] INFO: Add data frame as file chunks to data push job with id '7cc773ce-8e00-4169-96b4-cd054caf451b'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41144cc56e234c5fb51543971a7a96de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-04 09:06:21,970] INFO: Successfully upserted file chunk to data push job with id '7cc773ce-8e00-4169-96b4-cd054caf451b'\n",
      "[2023-06-04 09:06:22,133] INFO: Successfully triggered execution for data push job with id '7cc773ce-8e00-4169-96b4-cd054caf451b'\n",
      "[2023-06-04 09:06:22,135] INFO: Wait for execution of data push job with id '7cc773ce-8e00-4169-96b4-cd054caf451b'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5898b0645fa5433983f2e5a74f90037e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-04 09:06:38,578] INFO: Successfully created table 'PR_PLAYERS_NAME' in data pool\n",
      "[2023-06-04 09:06:39,033] INFO: Successfully deleted data push job with id '7cc773ce-8e00-4169-96b4-cd054caf451b'\n",
      "Upload of the Table Completed!\n",
      "_____________________________________________ \n",
      " \n",
      "\n",
      "Uploading of the 'PR_SCHEDULES' Table from Python to Celonis: \n",
      "\n",
      "[2023-06-04 09:06:43,489] INFO: Successfully created data push job with id 'eb8f8662-73a9-468d-a719-87bef133f8b6'\n",
      "[2023-06-04 09:06:43,490] INFO: Add data frame as file chunks to data push job with id 'eb8f8662-73a9-468d-a719-87bef133f8b6'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33091f7a6e3940f2917661e7a82c91d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-04 09:06:43,763] INFO: Successfully upserted file chunk to data push job with id 'eb8f8662-73a9-468d-a719-87bef133f8b6'\n",
      "[2023-06-04 09:06:43,925] INFO: Successfully triggered execution for data push job with id 'eb8f8662-73a9-468d-a719-87bef133f8b6'\n",
      "[2023-06-04 09:06:43,926] INFO: Wait for execution of data push job with id 'eb8f8662-73a9-468d-a719-87bef133f8b6'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23134847c644a2882ea5ea97564b49d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-04 09:06:58,401] INFO: Successfully created table 'PR_SCHEDULES' in data pool\n",
      "[2023-06-04 09:06:58,561] INFO: Successfully deleted data push job with id 'eb8f8662-73a9-468d-a719-87bef133f8b6'\n",
      "Upload of the Table Completed!\n",
      "_____________________________________________ \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dict_df_games = {\n",
    "    \"PR_DATA_GAMES\":     df_all_games,\n",
    "    \"PR_DATA_STANDINGS\": df_standing,\n",
    "    \"PR_PLAYERS_NAME\":   df_all_players,\n",
    "    \"PR_SCHEDULES\":      df_all_schedules\n",
    "}\n",
    "\n",
    "for lv_sql_table, lv_data_frame in dict_df_games.items():\n",
    "    \n",
    "    print(f\"Uploading of the '{lv_sql_table}' Table from Python to Celonis: \\n\")\n",
    "    \n",
    "    data_pool.create_table(table_name     = lv_sql_table,\n",
    "                           df             = lv_data_frame,\n",
    "                           drop_if_exists = True,\n",
    "                           force          = True)\n",
    "    \n",
    "    print(\"Upload of the Table Completed!\")\n",
    "    print(\"_\" * 45, \"\\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "164374ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-04 09:07:02,102] INFO: Successfully started execution for job with id 'f9300adf-cde5-43d8-bc44-eca7b355fda1'\n",
      "[2023-06-04 09:07:02,103] INFO: Wait for execution of job with id 'f9300adf-cde5-43d8-bc44-eca7b355fda1'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84c7efe0101484c893594fdc2488343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_job.name\n",
    "data_job.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0112995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model.reload()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
