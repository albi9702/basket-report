{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96fb8202",
<<<<<<< HEAD
   "metadata": {
    "tags": []
   },
=======
   "metadata": {},
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
   "source": [
    "# Scraping Ado\n",
    "\n",
    "This script shows how to scrape data from [LombardiaCanestro](https://lombardia.italiacanestro.it/). In this case, scrape the **Promozione - Girone E League**. Here the list of all Teams:\n",
    "\n",
    "* [Aurora Trezzo](https://lombardia.italiacanestro.it/Maschile/Squadra?id=42&tid=482).\n",
    "* [Posal Sesto San Giovanni](https://lombardia.italiacanestro.it/Maschile/Squadra?id=42&tid=483).\n",
    "* [Ado San Benedetto Milano](https://lombardia.italiacanestro.it/Maschile/Squadra?id=42&tid=484).\n",
    "* [CGB Brugherio](https://lombardia.italiacanestro.it/Maschile/Squadra?id=42&tid=485).\n",
    "* [Azzurri Niguardese](https://lombardia.italiacanestro.it/Maschile/Squadra?id=42&tid=486).\n",
    "* [Pallacanestro Carugate](https://lombardia.italiacanestro.it/Maschile/Squadra?id=42&tid=487).\n",
    "* [CBBA Olimpia Cologno](https://lombardia.italiacanestro.it/Maschile/Squadra?id=42&tid=488).\n",
    "* [Cesano Seveso](https://lombardia.italiacanestro.it/Maschile/Squadra?id=42&tid=489).\n",
    "* [Inzago Basket](https://lombardia.italiacanestro.it/Maschile/Squadra?id=42&tid=490).\n",
    "* [OSAL Novate](https://lombardia.italiacanestro.it/Maschile/Squadra?id=42&tid=491).\n",
    "* [Basket Ajaccio 1988](https://lombardia.italiacanestro.it/Maschile/Squadra?id=42&tid=492).\n",
    "* [Social OSA](https://lombardia.italiacanestro.it/Maschile/Squadra?id=42&tid=493).\n",
    "* [Basket San Rocco 2013 Seregno](https://lombardia.italiacanestro.it/Maschile/Squadra?id=42&tid=494).\n",
    "* [Ciesse Freebasket Milano](https://lombardia.italiacanestro.it/Maschile/Squadra?id=42&tid=495).\n",
    "* [ACLI Trecella](https://lombardia.italiacanestro.it/Maschile/Squadra?id=42&tid=496)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdff4c3",
   "metadata": {},
   "source": [
    "## Python Script\n",
    "\n",
    "The following cells shows the scrape script from the LombardiaCanestro Site. Scrape data:\n",
    "\n",
    "* *All games*, with this link: `https://lombardia.italiacanestro.it/Maschile/Partita?id=`***`<id_game>`***\n",
    "* *Standings*, with this link: `https://lombardia.italiacanestro.it/Maschile/Calendario?id=`***`42`***\n",
    "* *Rosters*, with this link: `https://lombardia.italiacanestro.it/Maschile/Roster?id=`***`42`***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eea041",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Libraries\n",
    "\n",
    "Used libraries:\n",
    "\n",
    "* `pandas`.\n",
    "* `pycelonis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10984b14",
<<<<<<< HEAD
   "metadata": {
    "tags": []
   },
=======
   "metadata": {},
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "[2023-05-06 16:42:29,611] WARNING: Your PyCelonis Version 2.0.0 is outdated (Newest Version: 2.0.3). Please upgrade the package via: pip install --extra-index-url=https://pypi.celonis.cloud/ pycelonis --upgrade\n"
=======
      "[2023-04-30 10:41:53,756] WARNING: Your PyCelonis Version 2.0.0 is outdated (Newest Version: 2.0.3). Please upgrade the package via: pip install --extra-index-url=https://pypi.celonis.cloud/ pycelonis --upgrade\n"
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "[2023-05-06 16:42:29,804] INFO: Initial connect successful! PyCelonis Version: 2.0.0\n",
      "[2023-05-06 16:42:29,995] INFO: `package-manager` permissions: ['EDIT_ALL_SPACES', 'MANAGE_PERMISSIONS', 'CREATE_SPACE', 'DELETE_ALL_SPACES']\n",
      "[2023-05-06 16:42:29,996] INFO: `workflows` permissions: ['EDIT_AGENTS', 'VIEW_AGENTS', 'REGISTER_AGENTS', 'MANAGE_PERMISSIONS']\n",
      "[2023-05-06 16:42:29,997] INFO: `task-mining` permissions: ['EDIT_CLIENT_SETTINGS', 'EDIT_USERS']\n",
      "[2023-05-06 16:42:29,997] INFO: `action-engine` permissions: ['CREATE_PROJECTS', 'MANAGE_SKILLS', 'ACCESS_ALL_PROJECTS', 'MY_INBOX']\n",
      "[2023-05-06 16:42:29,998] INFO: `team` permissions: ['MANAGE_AUDIT_LOGS', 'MANAGE_SSO_SETTINGS', 'USE_AUDIT_LOGS_API', 'MANAGE_ADOPTION_VIEWS', 'MANAGE_GENERAL_SETTINGS', 'MANAGE_GROUPS', 'MANAGE_APPLICATIONS', 'USE_STUDIO_ADOPTION_API', 'MANAGE_LOGIN_HISTORY', 'MANAGE_LICENSE_SETTINGS', 'USE_LOGIN_HISTORY_API', 'MANAGE_MEMBERS', 'MANAGE_UPLINK_INTEGRATIONS', 'MANAGE_PERMISSIONS', 'MANAGE_ADMIN_NOTIFICATIONS', 'IMPORT_MEMBERS', 'MANAGE_MEMBER_LOCKING_POLICY']\n",
      "[2023-05-06 16:42:29,998] INFO: `process-repository` permissions: ['CREATE_AND_MODIFY_CATEGORIES', 'USE_CATEGORIES', 'DELETE_EXISTING_CATEGORIES', 'MODIFY_EXISTING_CATEGORIES']\n",
      "[2023-05-06 16:42:29,999] INFO: `process-analytics` permissions: ['CREATE_WORKSPACE', 'MOVE_TO', 'DELETE_ALL_WORKSPACES', 'DELETE_ALL_ANALYSES', 'EDIT_ALL_ANALYSES', 'EDIT_ALL_WORKSPACES', 'USE_ALL_ANALYSES', 'CREATE_ANALYSES', 'MANAGE_PERMISSIONS', 'EXPORT_CONTENT']\n",
      "[2023-05-06 16:42:29,999] INFO: `transformation-center` permissions: ['MOVE_TO', 'EDIT_OBJECTIVE', 'VIEW_OBJECTIVE', 'CREATE_OBJECTIVE', 'MANAGE_PERMISSIONS', 'CREATE_KPI', 'EXPORT_CONTENT', 'DELETE_OBJECTIVE']\n",
      "[2023-05-06 16:42:30,000] INFO: `storage-manager` permissions: ['DELETE', 'CREATE', 'GET', 'ADMIN', 'LIST']\n",
      "[2023-05-06 16:42:30,000] INFO: `event-collection` permissions: ['EDIT_ALL_DATA_POOLS_RESTRICTED', 'USE_ALL_DATA_MODELS', 'VIEW_ALL_DATA_POOLS', 'CREATE_DATA_POOL', 'EDIT_ALL_DATA_POOLS']\n",
      "[2023-05-06 16:42:30,000] INFO: `user-provisioning` permissions: ['SCIM']\n",
      "[2023-05-06 16:42:30,001] INFO: `ml-workbench` permissions: ['DELETE_SCHEDULERS', 'EDIT_SCHEDULERS', 'USE_ALL_SCHEDULERS', 'USE_ALL_APPS', 'CREATE_SCHEDULERS', 'MANAGE_ALL_APPS', 'CREATE_WORKSPACES', 'MANAGE_SCHEDULERS_PERMISSIONS', 'VIEW_CONFIGURATION', 'CREATE_APPS', 'MANAGE_ALL_MLFLOWS', 'CREATE_MLFLOWS', 'USE_ALL_MLFLOWS', 'MANAGE_ALL_WORKSPACES']\n"
=======
      "[2023-04-30 10:41:53,967] INFO: Initial connect successful! PyCelonis Version: 2.0.0\n",
      "[2023-04-30 10:41:54,030] INFO: `package-manager` permissions: ['EDIT_ALL_SPACES', 'MANAGE_PERMISSIONS', 'CREATE_SPACE', 'DELETE_ALL_SPACES']\n",
      "[2023-04-30 10:41:54,032] INFO: `workflows` permissions: ['EDIT_AGENTS', 'VIEW_AGENTS', 'REGISTER_AGENTS', 'MANAGE_PERMISSIONS']\n",
      "[2023-04-30 10:41:54,033] INFO: `task-mining` permissions: ['EDIT_CLIENT_SETTINGS', 'EDIT_USERS']\n",
      "[2023-04-30 10:41:54,037] INFO: `action-engine` permissions: ['CREATE_PROJECTS', 'MANAGE_SKILLS', 'ACCESS_ALL_PROJECTS', 'MY_INBOX']\n",
      "[2023-04-30 10:41:54,038] INFO: `team` permissions: ['MANAGE_AUDIT_LOGS', 'MANAGE_SSO_SETTINGS', 'USE_AUDIT_LOGS_API', 'MANAGE_ADOPTION_VIEWS', 'MANAGE_GENERAL_SETTINGS', 'MANAGE_GROUPS', 'MANAGE_APPLICATIONS', 'USE_STUDIO_ADOPTION_API', 'MANAGE_LOGIN_HISTORY', 'MANAGE_LICENSE_SETTINGS', 'USE_LOGIN_HISTORY_API', 'MANAGE_MEMBERS', 'MANAGE_UPLINK_INTEGRATIONS', 'MANAGE_PERMISSIONS', 'MANAGE_ADMIN_NOTIFICATIONS', 'IMPORT_MEMBERS', 'MANAGE_MEMBER_LOCKING_POLICY']\n",
      "[2023-04-30 10:41:54,039] INFO: `process-repository` permissions: ['CREATE_AND_MODIFY_CATEGORIES', 'USE_CATEGORIES', 'DELETE_EXISTING_CATEGORIES', 'MODIFY_EXISTING_CATEGORIES']\n",
      "[2023-04-30 10:41:54,040] INFO: `process-analytics` permissions: ['CREATE_WORKSPACE', 'MOVE_TO', 'DELETE_ALL_WORKSPACES', 'DELETE_ALL_ANALYSES', 'EDIT_ALL_ANALYSES', 'EDIT_ALL_WORKSPACES', 'USE_ALL_ANALYSES', 'CREATE_ANALYSES', 'MANAGE_PERMISSIONS', 'EXPORT_CONTENT']\n",
      "[2023-04-30 10:41:54,041] INFO: `transformation-center` permissions: ['MOVE_TO', 'EDIT_OBJECTIVE', 'VIEW_OBJECTIVE', 'CREATE_OBJECTIVE', 'MANAGE_PERMISSIONS', 'CREATE_KPI', 'EXPORT_CONTENT', 'DELETE_OBJECTIVE']\n",
      "[2023-04-30 10:41:54,042] INFO: `storage-manager` permissions: ['DELETE', 'CREATE', 'GET', 'ADMIN', 'LIST']\n",
      "[2023-04-30 10:41:54,042] INFO: `event-collection` permissions: ['EDIT_ALL_DATA_POOLS_RESTRICTED', 'USE_ALL_DATA_MODELS', 'VIEW_ALL_DATA_POOLS', 'CREATE_DATA_POOL', 'EDIT_ALL_DATA_POOLS']\n",
      "[2023-04-30 10:41:54,043] INFO: `user-provisioning` permissions: ['SCIM']\n",
      "[2023-04-30 10:41:54,043] INFO: `ml-workbench` permissions: ['DELETE_SCHEDULERS', 'EDIT_SCHEDULERS', 'USE_ALL_SCHEDULERS', 'USE_ALL_APPS', 'CREATE_SCHEDULERS', 'MANAGE_ALL_APPS', 'CREATE_WORKSPACES', 'MANAGE_SCHEDULERS_PERMISSIONS', 'VIEW_CONFIGURATION', 'CREATE_APPS', 'MANAGE_ALL_MLFLOWS', 'CREATE_MLFLOWS', 'USE_ALL_MLFLOWS', 'MANAGE_ALL_WORKSPACES']\n"
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from   urllib.request import Request, urlopen\n",
<<<<<<< HEAD
    "from   tqdm import tqdm\n",
=======
    "from tqdm import tqdm\n",
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
    "\n",
    "#-- Celonis\n",
    "from pycelonis import get_celonis\n",
    "celonis = get_celonis(\n",
    "    base_url = \"alberto-filosa-protiviti-it.training.celonis.cloud\",\n",
    "    api_token = \"NzQ4Mzg3YjctNzkzNy00ZTFhLWE5ZTUtN2Y5NDk0MGVhYWJiOnlHK2xYb3NKRHpwTitGU053NUxOT2ZDZFZOUllKaXNsNWlUeGFwVnJ0UTc3\",\n",
    "    key_type = 'USER_KEY'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "8694c657-7dcc-4962-a1e9-b10ae7668418",
   "metadata": {},
   "source": [
    "### Scraping Webpage\n",
    "\n",
    "The `scraping_webpage` function takes in input the URL of the page to scrape the data (treated as *string*). It returns a DataFrame of the URL table.\n",
    "\n",
    "This function is nested in the following tables:\n",
    "\n",
    "* `scraping_table`.\n",
    "* `scraping_players`.\n",
    "* `scraping_standings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "827a970d-0818-4791-a852-4eb85bd5be0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scraping_webpage(lv_url):\n",
    "        \n",
    "    #-- Get URL\n",
    "    req = Request(lv_url, headers = {'User-Agent': 'Mozilla/5.0'})\n",
    "    webpage = urlopen(req).read()\n",
    "\n",
    "    #-- Get Table in HTML\n",
    "    df_scraped = pd.read_html(webpage)\n",
    "    \n",
    "    return df_scraped"
=======
   "id": "4cee2e50",
   "metadata": {},
   "source": [
    "### Games Scraping\n",
    "\n",
    "<!-- Inserire cosa fa la funzione -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b68ebc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scraping_table(ls_id_game):\n",
    "    \n",
    "    #-- Disable chained assignments\n",
    "    pd.options.mode.chained_assignment = None \n",
    "    \n",
    "    #-- Iniziate Empty DataFrame. Then, will be inserted for each ig_game the Game Result\n",
    "    df_all_games  = pd.DataFrame()\n",
    "    \n",
    "    for game in tqdm(ls_id_game):\n",
    "        \n",
    "        lv_url = f\"https://lombardia.italiacanestro.it/Maschile/Partita?id={game}\"\n",
    "        \n",
    "        #-- Get URL\n",
    "        req = Request(lv_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urlopen(req).read()\n",
    "            \n",
    "        #-- Get Current Game\n",
    "        df_single_game = pd.read_html(webpage)[0]\n",
    "        \n",
    "        #-- If not Played Yet, next to the next game\n",
    "        if len(df_single_game.index) < 5:\n",
    "            continue\n",
    "        \n",
    "        #-- List of Teams\n",
    "        ls_teams = df_single_game.loc[df_single_game[1] == \"PTS\"][0]\n",
    "        \n",
    "        #-----------------------\n",
    "        #-- Data Manipulation --\n",
    "        #-----------------------\n",
    "        \n",
    "        #-- Drop NAs (in a single URL there is one single Table to identify the Teams)\n",
    "        df_single_game_nona = df_single_game.dropna(how = \"all\")\n",
    "        \n",
    "        #-- Add Columns in the DataFrame\n",
    "        df_single_game_nona[\"Squadra\"]    = [ls_teams[0] if ls_teams.index[1] > row else ls_teams[ls_teams.index[1]] for row in range(0, df_single_game_nona.shape[0])]\n",
    "        df_single_game_nona[\"Squadra\"]    = df_single_game_nona[\"Squadra\"].str.title()\n",
    "        df_single_game_nona[\"Avversario\"] = [ls_teams[ls_teams.index[1]] if ls_teams.index[1] > row else ls_teams[0] for row in range(0, df_single_game_nona.shape[0])]\n",
    "        df_single_game_nona[\"Avversario\"] = df_single_game_nona[\"Avversario\"].str.title()\n",
    "        df_single_game_nona[\"Partita\"]    = [\"C\" if ls_teams.index[1] > row else \"T\" for row in range(0, df_single_game_nona.shape[0])]\n",
    "        df_single_game_nona[\"id_gara\"]    = game\n",
    "        \n",
    "        #-- Remove Header Rows (if they have PTS in the first column)\n",
    "        df_single_game_end = df_single_game_nona[df_single_game_nona[1] != 'PTS']\n",
    "        df_single_game_end.columns = [\"giocatore\", \"punti_totali\", \"tiri_liberi\",\n",
    "                                      \"due_punti\", \"tre_punti\", \"squadra\",\n",
    "                                      \"avversario\", \"partita\",\"id_gara\"]\n",
    "        \n",
    "        df_single_game_end[\"giocatore\"] = df_single_game_end[\"giocatore\"].str.title()\n",
    "        \n",
    "        #-- Concat Games\n",
    "        df_all_games = pd.concat([df_all_games, df_single_game_end])\n",
    "        \n",
    "    return df_all_games"
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "4cee2e50",
=======
   "id": "1841e001",
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
   "metadata": {},
   "source": [
    "### Games Scraping\n",
    "\n",
    "<!-- Inserire cosa fa la funzione -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
<<<<<<< HEAD
   "id": "3b68ebc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scraping_table(ls_id_game):\n",
    "    \n",
    "    #-- Disable chained assignments\n",
    "    pd.options.mode.chained_assignment = None \n",
    "    \n",
    "    #-- Iniziate Empty DataFrame. Then, will be inserted for each ig_game the Game Result\n",
    "    df_all_games  = pd.DataFrame()\n",
    "    \n",
    "    for game in tqdm(ls_id_game):\n",
    "        \n",
    "        lv_url = f\"https://lombardia.italiacanestro.it/Maschile/Partita?id={game}\"\n",
    "        \n",
    "        #-- Call the scraping_webpage function to scrape the Team Table\n",
    "        df_single_game = scraping_webpage(lv_url)[0]\n",
    "        \n",
    "        #-- If not Played Yet, next to the next game\n",
    "        if len(df_single_game.index) < 5:\n",
    "            continue\n",
    "        \n",
    "        #-- List of Teams\n",
    "        ls_teams = df_single_game.loc[df_single_game[1] == \"PTS\"][0]\n",
    "        \n",
    "        #-----------------------\n",
    "        #-- Data Manipulation --\n",
    "        #-----------------------\n",
    "        \n",
    "        #-- Drop NAs (in a single URL there is one single Table to identify the Teams)\n",
    "        df_single_game_nona = df_single_game.dropna(how = \"all\")\n",
    "        \n",
    "        #-- Add Columns in the DataFrame\n",
    "        df_single_game_nona[\"Squadra\"]    = [ls_teams[0] if ls_teams.index[1] > row else ls_teams[ls_teams.index[1]] for row in range(0, df_single_game_nona.shape[0])]\n",
    "        df_single_game_nona[\"Squadra\"]    = df_single_game_nona[\"Squadra\"].str.title()\n",
    "        df_single_game_nona[\"Avversario\"] = [ls_teams[ls_teams.index[1]] if ls_teams.index[1] > row else ls_teams[0] for row in range(0, df_single_game_nona.shape[0])]\n",
    "        df_single_game_nona[\"Avversario\"] = df_single_game_nona[\"Avversario\"].str.title()\n",
    "        df_single_game_nona[\"Partita\"]    = [\"C\" if ls_teams.index[1] > row else \"T\" for row in range(0, df_single_game_nona.shape[0])]\n",
    "        df_single_game_nona[\"id_gara\"]    = game\n",
    "        \n",
    "        #-- Remove Header Rows (if they have PTS in the first column)\n",
    "        df_single_game_end = df_single_game_nona[df_single_game_nona[1] != 'PTS']\n",
    "        df_single_game_end.columns = [\"giocatore\", \"punti_totali\", \"tiri_liberi\",\n",
    "                                      \"due_punti\", \"tre_punti\", \"squadra\",\n",
    "                                      \"avversario\", \"partita\",\"id_gara\"]\n",
    "        \n",
    "        df_single_game_end[\"giocatore\"] = df_single_game_end[\"giocatore\"].str.title()\n",
    "        \n",
    "        #-- Concat Games\n",
    "        df_all_games = pd.concat([df_all_games, df_single_game_end])\n",
    "        \n",
    "    return df_all_games"
=======
   "id": "79d9c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_players(id_roster):\n",
    "    \n",
    "    #-- Iniziate Empty DataFrame. Then, will be inserted for each ig_game the Game Result\n",
    "    df_all_players = pd.DataFrame()\n",
    "    \n",
    "    lv_url = f\"https://lombardia.italiacanestro.it/Maschile/Roster?id={id_roster}\"\n",
    "        \n",
    "    #-- Get URL\n",
    "    req = Request(lv_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    webpage = urlopen(req).read()\n",
    "\n",
    "    #-- Get Current Player\n",
    "    df_single_team = pd.read_html(webpage)\n",
    "    \n",
    "    for team in tqdm(df_single_team):\n",
    "    \n",
    "        team[\"Squadra\"] = team.columns[1].title()\n",
    "        team.columns = [\"Numero\", \"Giocatore\", \"Squadra\"]\n",
    "        df_all_players = pd.concat([df_all_players, team], axis = 0, ignore_index = True)\n",
    "    \n",
    "    df_all_players[\"Giocatore\"] = df_all_players[\"Giocatore\"].str.title()\n",
    "    \n",
    "    return df_all_players"
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "1841e001",
=======
   "id": "df9582c7",
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
   "metadata": {},
   "source": [
    "### Rosters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
<<<<<<< HEAD
   "id": "79d9c36b",
=======
   "id": "ca3e632e",
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "def scraping_players(id_roster):\n",
    "    \n",
    "    #-- Iniziate Empty DataFrame. Then, will be inserted for each ig_game the Game Result\n",
    "    df_all_players = pd.DataFrame()\n",
    "    \n",
    "    lv_url = f\"https://lombardia.italiacanestro.it/Maschile/Roster?id={id_roster}\"\n",
    "    \n",
    "    #-- Call the scraping_webpage function to scrape the Team Table\n",
    "    df_single_team = scraping_webpage(lv_url)\n",
    "    \n",
    "    for team in tqdm(df_single_team):\n",
    "    \n",
    "        team[\"Squadra\"] = team.columns[1].title()\n",
    "        team.columns = [\"Numero\", \"Giocatore\", \"Squadra\"]\n",
    "        df_all_players = pd.concat([df_all_players, team], axis = 0, ignore_index = True)\n",
    "    \n",
    "    df_all_players[\"Giocatore\"] = df_all_players[\"Giocatore\"].str.title()\n",
    "    \n",
    "    return df_all_players"
=======
    "def scraping_standings(id_calendar):\n",
    "    \n",
    "    lv_url = f\"https://lombardia.italiacanestro.it/Maschile/Calendario?id={id_calendar}\"\n",
    "        \n",
    "    #-- Get URL\n",
    "    req = Request(lv_url, headers = {'User-Agent': 'Mozilla/5.0'})\n",
    "    webpage = urlopen(req).read()\n",
    "    \n",
    "    df_standings = pd.read_html(webpage)[1]\n",
    "    \n",
    "    #-- Data String Manipulation\n",
    "    df_standings[\"CLASSIFICA\"] = df_standings[\"CLASSIFICA\"].str.title()\n",
    "    \n",
    "    df_standings.columns = [\"posizione\", \"squadra\", \"punti\", \"partite_giocate\",\n",
    "                            \"vittorie\",  \"sconfitte\", \"punti_fatti\", \"punti_subiti\"]\n",
    "    \n",
    "    return df_standings"
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "44a777e1-5d40-4d16-af56-eb724507f74e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Schedule Scraping\n",
    "\n",
    "The `scraping_schedule` takes in input the following variables:\n",
    "\n",
    "* `id_championship`: the identifier of the Championship.\n",
    "* `ls_turn`: the identifier of the turns (Andata - 2 and Ritorno - 3 in the URL)\n",
    "* `ls_round`: the number of the single round (from 1 to 15).\n",
    "\n",
    "It returns the Dataframe containing all Schedule of the Championship."
=======
   "id": "be1db6ac",
   "metadata": {},
   "source": [
    "## Upload to Celonis\n",
    "\n",
    "| DataFrame Name   | SQL Table Name      |\n",
    "|------------------|---------------------|\n",
    "| `df_all_games`   | `PR_DATA_GAMES`     |\n",
    "| `df_standing`    | `PR_DATA_STANDINGS` |\n",
    "| `df_all_players` | `PR_PLAYERS_NAME`   |"
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
<<<<<<< HEAD
   "id": "7b282e62-46ec-4129-9929-5ccf73c7aae8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scraping_schedule(id_championship, ls_turn, ls_round):\n",
    "    \n",
    "    #-- Disable chained assignments\n",
    "    pd.options.mode.chained_assignment = None \n",
    "    \n",
    "    #-- Iniziate Empty DataFrame. Then, will be inserted for each ig_game the Game Result\n",
    "    df_all_schedules  = pd.DataFrame()\n",
    "    \n",
    "    for turns in ls_turn:\n",
    "        print(f\"Scraping Round {turns - 1} ...\")\n",
    "        \n",
    "        for rounds in tqdm(ls_round):\n",
    "            \n",
    "            lv_url = f\"https://lombardia.italiacanestro.it/Maschile/Calendario?handler=Change&ChampionshipId={id_championship}&TurnId={turns}&TurnNm={rounds}\"\n",
    "            \n",
    "            df_single_round = scraping_webpage(lv_url)[0]\n",
    "            df_single_round_tidy = pd.DataFrame()\n",
    "            df_single_round_tidy[\"Data\"] = df_single_round[0][df_single_round.index % 4 == 0].reset_index(drop = True).str.title()\n",
    "            df_single_round_tidy[\"Squadra\"] = df_single_round[0][(df_single_round.index - 1) % 4 == 0].reset_index(drop = True).str.replace(r'\\([^)]*\\)', '', regex = True).str.title()\n",
    "            df_single_round_tidy[\"Risultato\"] = df_single_round[1][(df_single_round.index - 1) % 4 == 0].reset_index(drop = True)\n",
    "            df_single_round_tidy[\"Avversario\"] = df_single_round[2][(df_single_round.index - 1) % 4 == 0].reset_index(drop = True).str.replace(r'\\([^)]*\\)', '', regex = True).str.title()\n",
    "            df_single_round_tidy[\"Girone\"] = turns - 1\n",
    "            df_single_round_tidy[\"Turno\"] = rounds\n",
    "            df_single_round_tidy.head()\n",
    "            \n",
    "            #-- Concat Games\n",
    "            df_all_schedules = pd.concat([df_all_schedules, df_single_round_tidy])\n",
    "            \n",
    "    return df_all_schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9582c7",
   "metadata": {},
   "source": [
    "### Standings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca3e632e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scraping_standings(id_calendar):\n",
    "    \n",
    "    lv_url = f\"https://lombardia.italiacanestro.it/Maschile/Calendario?id={id_calendar}\"\n",
    "\n",
    "    #-- Call the scraping_webpage function to scrape the Standing Table\n",
    "    df_standings = scraping_webpage(lv_url)[1]\n",
    "\n",
    "    #-- Data String Manipulation\n",
    "    df_standings[\"CLASSIFICA\"] = df_standings[\"CLASSIFICA\"].str.title()\n",
    "\n",
    "    df_standings.columns = [\"posizione\", \"squadra\", \"punti\", \"partite_giocate\",\n",
    "                            \"vittorie\",  \"sconfitte\", \"punti_fatti\", \"punti_subiti\"]\n",
    "\n",
    "    return df_standings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1db6ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Upload to Celonis\n",
    "\n",
    "| DataFrame Name     | SQL Table Name      |\n",
    "|--------------------|---------------------|\n",
    "| `df_all_games`     | `PR_DATA_GAMES`     |\n",
    "| `df_standing`      | `PR_DATA_STANDINGS` |\n",
    "| `df_all_players`   | `PR_PLAYERS_NAME`   |\n",
    "| `df_all_schedules` | `PR_SCHEDULES`      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21d28a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading All Games ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 206/206 [00:52<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading All Players ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 1266.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Standins ...\n",
      "Downloading All Schedule ...\n",
      "Scraping Round 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:05<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Round 2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:06<00:00,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.67 s, sys: 358 ms, total: 7.03 s\n",
      "Wall time: 1min 5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
=======
   "id": "21d28a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading All Games ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 206/206 [00:44<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading All Players ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 1075.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.88 s, sys: 360 ms, total: 7.24 s\n",
      "Wall time: 45.4 s\n"
     ]
    }
   ],
   "source": [
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
    "%%time\n",
    "\n",
    "print(\"Downloading All Games ...\")\n",
    "df_all_games   = scraping_table(np.arange(5508, 5714))\n",
    "\n",
    "print(\"Downloading All Players ...\")\n",
    "df_all_players = scraping_players(42)\n",
    "\n",
<<<<<<< HEAD
    "print(\"Downloading Standins ...\")\n",
    "df_standing    = scraping_standings(42)\n",
    "\n",
    "print(\"Downloading All Schedule ...\")\n",
    "df_all_schedules = scraping_schedule(42, np.arange(2, 4), np.arange(1, 16))"
=======
    "df_standing    = scraping_standings(42)"
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 6,
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
   "id": "ab9761a2",
   "metadata": {
    "tags": []
   },
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected the 'Basket - Scraping Data' Data Pool and the 'Data Model - Promozione - Girone E' Data Model\n"
     ]
    }
   ],
   "source": [
    "#-- Selecting Data Pool, Data Model and Data Job\n",
    "data_pool = celonis.data_integration.get_data_pool(\"26a8fa87-21b1-4850-9447-48c2e6a171fc\")\n",
    "data_model = data_pool.get_data_model(\"9fb8576b-a8f6-4f71-9cb9-4722bafa7a92\")\n",
    "print(f\"Selected the '{data_pool.name}' Data Pool and the '{data_model.name}' Data Model\")\n",
    "\n",
    "data_job = data_pool.get_job(\"f9300adf-cde5-43d8-bc44-eca7b355fda1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "feeabc14",
   "metadata": {
    "tags": []
   },
=======
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Uploading of the 'PR_DATA_GAMES' Table from Python to Celonis: \n",
      "\n",
      "[2023-05-06 16:43:38,108] INFO: Successfully created data push job with id 'b1827bb6-2abe-4e24-87a9-f9539de8bb3f'\n",
      "[2023-05-06 16:43:38,109] INFO: Add data frame as file chunks to data push job with id 'b1827bb6-2abe-4e24-87a9-f9539de8bb3f'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83013652d1ca46d5a9e03602c4e9c72b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
=======
      "Selected the 'Basket - Scraping Data' Data Pool and the 'Data Model - Promozione - Girone E' Data Model\n"
     ]
    }
   ],
   "source": [
    "#-- Selecting Data Pool, Data Model and Data Job\n",
    "data_pool = celonis.data_integration.get_data_pool(\"26a8fa87-21b1-4850-9447-48c2e6a171fc\")\n",
    "data_model = data_pool.get_data_model(\"9fb8576b-a8f6-4f71-9cb9-4722bafa7a92\")\n",
    "print(f\"Selected the '{data_pool.name}' Data Pool and the '{data_model.name}' Data Model\")\n",
    "\n",
    "data_job = data_pool.get_job(\"f9300adf-cde5-43d8-bc44-eca7b355fda1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feeabc14",
   "metadata": {
    "tags": []
   },
   "outputs": [
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "[2023-05-06 16:43:38,785] INFO: Successfully upserted file chunk to data push job with id 'b1827bb6-2abe-4e24-87a9-f9539de8bb3f'\n",
      "[2023-05-06 16:43:38,983] INFO: Successfully triggered execution for data push job with id 'b1827bb6-2abe-4e24-87a9-f9539de8bb3f'\n",
      "[2023-05-06 16:43:38,984] INFO: Wait for execution of data push job with id 'b1827bb6-2abe-4e24-87a9-f9539de8bb3f'\n"
=======
      "Uploading of the 'PR_DATA_GAMES' Table from Python to Celonis: \n",
      "\n",
      "[2023-04-30 10:42:47,632] INFO: Successfully created data push job with id '57b9b6ba-e65d-4485-8dee-3557f5566e77'\n",
      "[2023-04-30 10:42:47,634] INFO: Add data frame as file chunks to data push job with id '57b9b6ba-e65d-4485-8dee-3557f5566e77'\n"
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
       "model_id": "97eb407a0f7445529287d35988aac920",
=======
       "model_id": "dfd1e19a0c554049afdc24566d6dd56e",
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
<<<<<<< HEAD
       "0it [00:00, ?it/s]"
=======
       "  0%|          | 0/1 [00:00<?, ?it/s]"
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "[2023-05-06 16:43:49,292] INFO: Successfully created table 'PR_DATA_GAMES' in data pool\n",
      "[2023-05-06 16:43:49,439] INFO: Successfully deleted data push job with id 'b1827bb6-2abe-4e24-87a9-f9539de8bb3f'\n",
      "Upload of the Table Completed!\n",
      "_____________________________________________ \n",
      " \n",
      "\n",
      "Uploading of the 'PR_DATA_STANDINGS' Table from Python to Celonis: \n",
      "\n",
      "[2023-05-06 16:43:52,221] INFO: Successfully created data push job with id '9823dc04-236b-4122-9302-b43601c389c1'\n",
      "[2023-05-06 16:43:52,222] INFO: Add data frame as file chunks to data push job with id '9823dc04-236b-4122-9302-b43601c389c1'\n"
=======
      "[2023-04-30 10:42:47,935] INFO: Successfully upserted file chunk to data push job with id '57b9b6ba-e65d-4485-8dee-3557f5566e77'\n",
      "[2023-04-30 10:42:48,115] INFO: Successfully triggered execution for data push job with id '57b9b6ba-e65d-4485-8dee-3557f5566e77'\n",
      "[2023-04-30 10:42:48,117] INFO: Wait for execution of data push job with id '57b9b6ba-e65d-4485-8dee-3557f5566e77'\n"
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
       "model_id": "2f6773abe5034ab79836cefe59ccf94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-06 16:43:52,435] INFO: Successfully upserted file chunk to data push job with id '9823dc04-236b-4122-9302-b43601c389c1'\n",
      "[2023-05-06 16:43:52,601] INFO: Successfully triggered execution for data push job with id '9823dc04-236b-4122-9302-b43601c389c1'\n",
      "[2023-05-06 16:43:52,603] INFO: Wait for execution of data push job with id '9823dc04-236b-4122-9302-b43601c389c1'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d5b1cc7ff7646589b81d2416b225cb4",
=======
       "model_id": "d9faab9283824ee6b34e90926bddb9f0",
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "[2023-05-06 16:44:04,971] INFO: Successfully created table 'PR_DATA_STANDINGS' in data pool\n",
      "[2023-05-06 16:44:05,326] INFO: Successfully deleted data push job with id '9823dc04-236b-4122-9302-b43601c389c1'\n",
=======
      "[2023-04-30 10:43:08,684] INFO: Successfully created table 'PR_DATA_GAMES' in data pool\n",
      "[2023-04-30 10:43:08,821] INFO: Successfully deleted data push job with id '57b9b6ba-e65d-4485-8dee-3557f5566e77'\n",
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
      "Upload of the Table Completed!\n",
      "_____________________________________________ \n",
      " \n",
      "\n",
<<<<<<< HEAD
      "Uploading of the 'PR_PLAYERS_NAME' Table from Python to Celonis: \n",
      "\n",
      "[2023-05-06 16:44:08,434] INFO: Successfully created data push job with id 'caddd955-93d4-4a87-8fc5-85a04b78ff36'\n",
      "[2023-05-06 16:44:08,435] INFO: Add data frame as file chunks to data push job with id 'caddd955-93d4-4a87-8fc5-85a04b78ff36'\n"
=======
      "Uploading of the 'PR_DATA_STANDINGS' Table from Python to Celonis: \n",
      "\n",
      "[2023-04-30 10:43:15,279] INFO: Successfully created data push job with id '26cb7022-e36f-4036-97e5-e980ecef2a71'\n",
      "[2023-04-30 10:43:15,281] INFO: Add data frame as file chunks to data push job with id '26cb7022-e36f-4036-97e5-e980ecef2a71'\n"
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
       "model_id": "ffbb1f292b704a83a496689ddd0d42d9",
=======
       "model_id": "ca867df73f6c4ec0a568bccf531ad48e",
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "[2023-05-06 16:44:08,653] INFO: Successfully upserted file chunk to data push job with id 'caddd955-93d4-4a87-8fc5-85a04b78ff36'\n",
      "[2023-05-06 16:44:08,982] INFO: Successfully triggered execution for data push job with id 'caddd955-93d4-4a87-8fc5-85a04b78ff36'\n",
      "[2023-05-06 16:44:08,983] INFO: Wait for execution of data push job with id 'caddd955-93d4-4a87-8fc5-85a04b78ff36'\n"
=======
      "[2023-04-30 10:43:15,558] INFO: Successfully upserted file chunk to data push job with id '26cb7022-e36f-4036-97e5-e980ecef2a71'\n",
      "[2023-04-30 10:43:15,730] INFO: Successfully triggered execution for data push job with id '26cb7022-e36f-4036-97e5-e980ecef2a71'\n",
      "[2023-04-30 10:43:15,732] INFO: Wait for execution of data push job with id '26cb7022-e36f-4036-97e5-e980ecef2a71'\n"
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
       "model_id": "979c31f5fc854e39bc3a05d2924c48cc",
=======
       "model_id": "0b8259e4b0ee4c6b9f7c42803ad27055",
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "[2023-05-06 16:44:21,466] INFO: Successfully created table 'PR_PLAYERS_NAME' in data pool\n",
      "[2023-05-06 16:44:22,347] INFO: Successfully deleted data push job with id 'caddd955-93d4-4a87-8fc5-85a04b78ff36'\n",
=======
      "[2023-04-30 10:43:30,138] INFO: Successfully created table 'PR_DATA_STANDINGS' in data pool\n",
      "[2023-04-30 10:43:30,284] INFO: Successfully deleted data push job with id '26cb7022-e36f-4036-97e5-e980ecef2a71'\n",
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
      "Upload of the Table Completed!\n",
      "_____________________________________________ \n",
      " \n",
      "\n",
<<<<<<< HEAD
      "Uploading of the 'PR_SCHEDULES' Table from Python to Celonis: \n",
      "\n",
      "[2023-05-06 16:44:25,135] INFO: Successfully created data push job with id 'c92225fb-cffc-4ab4-8be6-cdee6a9d17d2'\n",
      "[2023-05-06 16:44:25,137] INFO: Add data frame as file chunks to data push job with id 'c92225fb-cffc-4ab4-8be6-cdee6a9d17d2'\n"
=======
      "Uploading of the 'PR_PLAYERS_NAME' Table from Python to Celonis: \n",
      "\n",
      "[2023-04-30 10:43:42,680] INFO: Successfully created data push job with id '30ad0b67-0b7d-4cdb-965a-3cec7ea17725'\n",
      "[2023-04-30 10:43:42,681] INFO: Add data frame as file chunks to data push job with id '30ad0b67-0b7d-4cdb-965a-3cec7ea17725'\n"
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
       "model_id": "4a39d21c6eaa4154ac8dca5af72e9d5d",
=======
       "model_id": "afab12b858874f5c8fb7aa4c43ed426a",
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "[2023-05-06 16:44:25,381] INFO: Successfully upserted file chunk to data push job with id 'c92225fb-cffc-4ab4-8be6-cdee6a9d17d2'\n",
      "[2023-05-06 16:44:25,543] INFO: Successfully triggered execution for data push job with id 'c92225fb-cffc-4ab4-8be6-cdee6a9d17d2'\n",
      "[2023-05-06 16:44:25,544] INFO: Wait for execution of data push job with id 'c92225fb-cffc-4ab4-8be6-cdee6a9d17d2'\n"
=======
      "[2023-04-30 10:43:42,965] INFO: Successfully upserted file chunk to data push job with id '30ad0b67-0b7d-4cdb-965a-3cec7ea17725'\n",
      "[2023-04-30 10:43:43,132] INFO: Successfully triggered execution for data push job with id '30ad0b67-0b7d-4cdb-965a-3cec7ea17725'\n",
      "[2023-04-30 10:43:43,134] INFO: Wait for execution of data push job with id '30ad0b67-0b7d-4cdb-965a-3cec7ea17725'\n"
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
       "model_id": "52b80739e46445aa9bdaae39e94e0e35",
=======
       "model_id": "8dae93d8d0ec45c285e0cbb8ba2a2309",
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "[2023-05-06 16:44:39,993] INFO: Successfully created table 'PR_SCHEDULES' in data pool\n",
      "[2023-05-06 16:44:40,129] INFO: Successfully deleted data push job with id 'c92225fb-cffc-4ab4-8be6-cdee6a9d17d2'\n",
=======
      "[2023-04-30 10:43:55,480] INFO: Successfully created table 'PR_PLAYERS_NAME' in data pool\n",
      "[2023-04-30 10:43:55,648] INFO: Successfully deleted data push job with id '30ad0b67-0b7d-4cdb-965a-3cec7ea17725'\n",
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
      "Upload of the Table Completed!\n",
      "_____________________________________________ \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dict_df_games = {\n",
    "    \"PR_DATA_GAMES\":     df_all_games,\n",
    "    \"PR_DATA_STANDINGS\": df_standing,\n",
<<<<<<< HEAD
    "    \"PR_PLAYERS_NAME\":   df_all_players,\n",
    "    \"PR_SCHEDULES\":      df_all_schedules\n",
=======
    "    \"PR_PLAYERS_NAME\":   df_all_players\n",
>>>>>>> 8a1c24753f1d62e91ee12f36fa9f32c3a2e8d151
    "}\n",
    "\n",
    "for lv_sql_table, lv_data_frame in dict_df_games.items():\n",
    "    \n",
    "    print(f\"Uploading of the '{lv_sql_table}' Table from Python to Celonis: \\n\")\n",
    "    \n",
    "    data_pool.create_table(table_name     = lv_sql_table,\n",
    "                           df             = lv_data_frame,\n",
    "                           drop_if_exists = True,\n",
    "                           force          = True)\n",
    "    \n",
    "    print(\"Upload of the Table Completed!\")\n",
    "    print(\"_\" * 45, \"\\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a508a410",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_job.name\n",
    "data_job.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1696c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model.reload()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
